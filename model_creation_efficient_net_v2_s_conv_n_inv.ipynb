{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sashankh-ravi/anaconda3/envs/birdappenv1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from timm.models.efficientnet import efficientnetv2_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified efficientnetv2_s architecture (Convlution + Involution Kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open source Involution (https://github.com/shuuchen/involution.pytorch/blob/main/involution.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Involution(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of `Involution: Inverting the Inherence of Convolution for Visual Recognition`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, groups=1, kernel_size=3, stride=1, reduction_ratio=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        channels_reduced = max(1, in_channels // reduction_ratio)\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels_reduced, 1),\n",
    "            nn.BatchNorm2d(channels_reduced),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.span = nn.Conv2d(channels_reduced, kernel_size * kernel_size * groups, 1)\n",
    "        self.unfold = nn.Unfold(kernel_size, padding=padding, stride=stride)\n",
    "        \n",
    "        self.resampling = None if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "\n",
    "    @classmethod\n",
    "    def get_name(cls):\n",
    "        \"\"\"\n",
    "        Return this layer name.\n",
    "\n",
    "        Returns:\n",
    "            str: layer name.\n",
    "        \"\"\"\n",
    "        return 'Involution'\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Calculate Involution.\n",
    "\n",
    "        override function from PyTorch.\n",
    "        \"\"\"\n",
    "        _, _, height, width = input_tensor.size()\n",
    "        if self.stride > 1:\n",
    "            out_size = lambda x: (x + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "            height, width = out_size(height), out_size(width)\n",
    "        uf_x = rearrange(self.unfold(input_tensor), 'b (g d k j) (h w) -> b g d (k j) h w',\n",
    "                         g=self.groups, k=self.kernel_size, j=self.kernel_size, h=height, w=width)\n",
    "\n",
    "        if self.stride > 1:\n",
    "            input_tensor = F.adaptive_avg_pool2d(input_tensor, (height, width))\n",
    "        kernel = rearrange(self.span(self.reduce(input_tensor)), 'b (k j g) h w -> b g (k j) h w',\n",
    "                           k=self.kernel_size, j=self.kernel_size)\n",
    "\n",
    "        out = rearrange(torch.einsum('bgdxhw, bgxhw -> bgdhw', uf_x, kernel), 'b g d h w -> b (g d) h w')\n",
    "        \n",
    "        if self.resampling:\n",
    "            out = self.resampling(out)\n",
    "            \n",
    "        return out.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify all Conv2D within the EfficientNet-v2-s architecture to Conv2D + Inv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvInvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvInvolutionBlock, self).__init__()\n",
    "\n",
    "        # Convolution block\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.conv_act = nn.SiLU(inplace=True)\n",
    "\n",
    "        # Involution block\n",
    "        self.involution = Involution(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        self.inv_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.inv_act = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution block\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_bn(x)\n",
    "        x = self.conv_act(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Apply involution block\n",
    "        x = self.involution(x)\n",
    "        x = self.inv_bn(x)\n",
    "        x = self.inv_act(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the updated EfficientNetV2SWithInvolution class\n",
    "class EfficientNetV2SWithInvolution(nn.Module):\n",
    "    def __init__(self, num_classes):  # Example for 86 classes (birds)\n",
    "        super(EfficientNetV2SWithInvolution, self).__init__()\n",
    "\n",
    "        # Load the base EfficientNetV2-S model\n",
    "        base_model = efficientnetv2_s(pretrained=False)  # Set pretrained=False to initialize from scratch\n",
    "\n",
    "        # Get the stem and features from the original EfficientNetV2-S\n",
    "        self.stem = base_model.conv_stem  # This is the stem (initial convolution)\n",
    "        self.blocks = base_model.blocks  # Blocks contain all the convolutional layers\n",
    "\n",
    "        # Replace Conv2d inside each block with ConvInvolutionBlock every 5th occurrence\n",
    "        self.modified_blocks = nn.ModuleList()\n",
    "        conv_count = 0\n",
    "\n",
    "        for block in self.blocks:\n",
    "            # Recursively replace Conv2d in block\n",
    "            block_children = self._replace_conv_in_block(block, conv_count)\n",
    "            self.modified_blocks.append(nn.Sequential(*block_children))\n",
    "\n",
    "        # Final classifier layers\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _replace_conv_in_block(self, block, conv_count):\n",
    "        block_children = list(block.children())\n",
    "        modified_block_children = []\n",
    "\n",
    "        for sub_block in block_children:\n",
    "            sub_block_children = list(sub_block.children())\n",
    "            modified_sub_block_children = []\n",
    "\n",
    "            for sub_block_child in sub_block_children:\n",
    "                if isinstance(sub_block_child, nn.Conv2d):\n",
    "                    conv_count += 1  # Increment Conv2d counter\n",
    "                    if conv_count % 10 == 0:\n",
    "                        # Replace the Conv2d layer with ConvInvolutionBlock every 5th occurrence\n",
    "                        kernel_size = sub_block_child.kernel_size[0]\n",
    "                        stride = sub_block_child.stride[0]\n",
    "                        padding = sub_block_child.padding[0]\n",
    "                        modified_sub_block = ConvInvolutionBlock(sub_block_child.in_channels, sub_block_child.out_channels, kernel_size, stride, padding)\n",
    "                        modified_sub_block_children.append(modified_sub_block)\n",
    "                    else:\n",
    "                        # Keep the original Conv2d layer\n",
    "                        modified_sub_block_children.append(sub_block_child)\n",
    "                elif isinstance(sub_block_child, nn.Sequential):\n",
    "                    # Recursively check inside the Sequential blocks\n",
    "                    modified_sub_block_children.append(self._replace_conv_in_block(sub_block_child, conv_count))\n",
    "                else:\n",
    "                    # If the submodule is not Conv2d, keep it as it is\n",
    "                    modified_sub_block_children.append(sub_block_child)\n",
    "\n",
    "            # Once all sub-blocks have been processed, add the modified block\n",
    "            modified_block_children.append(nn.Sequential(*modified_sub_block_children))\n",
    "\n",
    "        return modified_block_children\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through stem\n",
    "        x = self.stem(x)\n",
    "\n",
    "        # Forward pass through modified blocks\n",
    "        for block in self.modified_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global Average Pooling and classifier\n",
    "        x = x.mean([2, 3])  # Global Average Pooling\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetV2SWithInvolution(num_classes=1486)  # Dynamically set number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check torch summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional: Collect garbage (useful if memory is fragmented)\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Assuming your model is on the GPU\n",
    "# model = efficientnetv2_s(pretrained=False)\n",
    "model = EfficientNetV2SWithInvolution(num_classes=1486)  # Dynamically set number of classes\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)  # Ensure model is on GPU\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional: Collect garbage (useful if memory is fragmented)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32  # Batch size\n",
    "epochs = 10  # Set the number of epochs\n",
    "\n",
    "# Dataset paths\n",
    "train_dir = \"/home/sashankh-ravi/Documents/Datasets/iNet_Bird_Small/train_transformed\"\n",
    "val_dir = \"/home/sashankh-ravi/Documents/Datasets/iNet_Bird_Small/val_transformed\"\n",
    "\n",
    "# Dynamically fetch number of classes from the folder structure (number of subfolders)\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "\n",
    "# Image resizing and transformation (only resizing to 256x256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Custom dataset loading\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "# Create DataLoader with shuffling enabled for the training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the model (assuming EfficientNetV2SWithInvolution is defined elsewhere)\n",
    "model = EfficientNetV2SWithInvolution(num_classes=1486)\n",
    "# model = efficientnetv2_s(pretrained=False)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (e.g., Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the gradient scaler for FP16 precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass using mixed precision (autocast)\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):  # Use autocast for FP16 precision\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update the model weights using FP16 gradients and scaling\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_preds / total_preds\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():  # Use autocast for FP16 precision\n",
    "                outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "            # Track accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct_preds / total_preds\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Plot the cumulative training and validation loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label=\"Training Loss\", color='blue', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label=\"Validation Loss\", color='red', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cumulative Loss')\n",
    "plt.title('Cumulative Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdappenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
