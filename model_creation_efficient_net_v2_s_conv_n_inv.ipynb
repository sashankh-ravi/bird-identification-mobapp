{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sashankhravi/miniconda3/envs/birdapp1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from timm.models.efficientnet import efficientnetv2_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an architercture like EfficientNetV2-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Leverage open-source code for Involution\n",
    "2. 1/10th the size of Timm EfficientNetV2-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class SqueezeExcite(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation (SE) block enhances feature representation\n",
    "    by adaptively recalibrating channel-wise feature responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, se_ratio=0.25):\n",
    "        super().__init__()\n",
    "        reduced_ch = max(1, int(in_ch * se_ratio))\n",
    "        self.fc1 = nn.utils.weight_norm(nn.Conv2d(in_ch, reduced_ch, 1))\n",
    "        self.fc2 = nn.utils.weight_norm(nn.Conv2d(reduced_ch, in_ch, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        se = F.adaptive_avg_pool2d(x, 1)\n",
    "        se = F.silu(self.fc1(se))\n",
    "        se = torch.sigmoid(self.fc2(se))\n",
    "        return x * se\n",
    "\n",
    "class Involution(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of `Involution: Inverting the Inherence of Convolution for Visual Recognition`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, groups=1, kernel_size=3, stride=1, reduction_ratio=2):\n",
    "        super().__init__()\n",
    "        channels_reduced = max(1, in_channels // reduction_ratio)\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Conv2d(in_channels, channels_reduced, 1)),\n",
    "            nn.BatchNorm2d(channels_reduced),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.span = nn.utils.weight_norm(nn.Conv2d(channels_reduced, kernel_size * kernel_size * groups, 1))\n",
    "        self.unfold = nn.Unfold(kernel_size, padding=padding, stride=stride)\n",
    "        \n",
    "        self.resampling = None if in_channels == out_channels else nn.utils.weight_norm(nn.Conv2d(in_channels, out_channels, 1))\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # print(f\"Involution Input: {input_tensor.shape}\")\n",
    "        _, _, height, width = input_tensor.size()\n",
    "        if self.stride > 1:\n",
    "            out_size = lambda x: (x + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "            height, width = out_size(height), out_size(width)\n",
    "        uf_x = rearrange(self.unfold(input_tensor), 'b (g d k j) (h w) -> b g d (k j) h w',\n",
    "                         g=self.groups, k=self.kernel_size, j=self.kernel_size, h=height, w=width)\n",
    "\n",
    "        if self.stride > 1:\n",
    "            input_tensor = F.adaptive_avg_pool2d(input_tensor, (height, width))\n",
    "        kernel = rearrange(self.span(self.reduce(input_tensor)), 'b (k j g) h w -> b g (k j) h w',\n",
    "                           k=self.kernel_size, j=self.kernel_size)\n",
    "\n",
    "        out = rearrange(torch.einsum('bgdxhw, bgxhw -> bgdhw', uf_x, kernel), 'b g d h w -> b (g d) h w')\n",
    "        \n",
    "        if self.resampling:\n",
    "            out = self.resampling(out)\n",
    "        \n",
    "        # print(f\"Involution Output: {out.shape}\")\n",
    "        return out.contiguous()\n",
    "\n",
    "class ConvInvolutionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This block combines both convolution and involution to leverage their strengths.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, expansion=1):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(in_channels * expansion)\n",
    "        self.expand = nn.utils.weight_norm(nn.Conv2d(in_channels, hidden_dim, 1)) if expansion > 1 else nn.Identity()\n",
    "        self.expand_bn = nn.BatchNorm2d(hidden_dim) if expansion > 1 else nn.Identity()\n",
    "        self.expand_act = nn.SiLU(inplace=True) if expansion > 1 else nn.Identity()\n",
    "        \n",
    "        self.conv = nn.utils.weight_norm(nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim))\n",
    "        self.conv_bn = nn.BatchNorm2d(hidden_dim)\n",
    "        self.conv_act = nn.SiLU(inplace=True)\n",
    "        \n",
    "        self.involution = Involution(hidden_dim, out_channels, kernel_size=kernel_size, stride=1)\n",
    "        self.inv_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.inv_act = nn.SiLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"ConvInvolutionBlock Input: {x.shape}\")\n",
    "        x = self.expand(x)\n",
    "        x = self.expand_bn(x)\n",
    "        x = self.expand_act(x)\n",
    "        # print(f\"After Expansion: {x.shape}\")\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_bn(x)\n",
    "        x = self.conv_act(x)\n",
    "        # print(f\"After Conv: {x.shape}\")\n",
    "        x = self.involution(x)\n",
    "        x = self.inv_bn(x)\n",
    "        x = self.inv_act(x)\n",
    "        # print(f\"After Involution: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, expansion, se_ratio, num_repeats=1, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_repeats):\n",
    "            layers.append(ConvInvolutionBlock(in_ch, out_ch, kernel_size, stride, kernel_size//2, expansion))\n",
    "            in_ch = out_ch\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            # print(f\"MBConv Input: {x.shape}\")\n",
    "            x = block(x)\n",
    "            # print(f\"MBConv Output: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class EfficientNetV2S_WithInvolution(nn.Module):\n",
    "    def __init__(self, num_classes=1486):\n",
    "        super().__init__()\n",
    "        self.stem = ConvInvolutionBlock(3, 24, 3, stride=2, padding=1, expansion=1)\n",
    "        self.blocks = nn.Sequential(\n",
    "            MBConv(24, 48, 3, 2, 4, 0.0, num_repeats=5),\n",
    "            MBConv(48, 64, 3, 2, 4, 0.25, num_repeats=5),\n",
    "            MBConv(64, 128, 3, 2, 4, 0.25, num_repeats=5),\n",
    "            MBConv(128, 160, 3, 1, 6, 0.25, num_repeats=3),\n",
    "            MBConv(160, 256, 3, 2, 6, 0.25, num_repeats=1)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check torch summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional: Collect garbage (useful if memory is fragmented)\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Assuming your model is on the GPU\n",
    "# model = efficientnetv2_s(pretrained=False)\n",
    "model = EfficientNetV2S_WithInvolution(num_classes=1486)  # Dynamically set number of classes\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)  # Ensure model is on GPU\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional: Collect garbage (useful if memory is fragmented)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sashankhravi/miniconda3/envs/birdapp1/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/11096], Train Loss: 7.3095\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_run = 1\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 150  # Batch size\n",
    "epochs = 100  # Set the number of epochs\n",
    "grad_clip = 1.0  # Maximum gradient norm\n",
    "\n",
    "# Dataset paths\n",
    "train_dir = \"/home/sashankhravi/Datasets/inatbirds100k/train_transformed\"\n",
    "val_dir = \"/home/sashankhravi/Datasets/inatbirds100k/val_transformed\"\n",
    "\n",
    "# Dynamically fetch number of classes from the folder structure (number of subfolders)\n",
    "num_classes = len(os.listdir(train_dir))\n",
    "\n",
    "# Image resizing and transformation (only resizing to 256x256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Custom dataset loading\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "# Create DataLoader with the sampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the model (assuming EfficientNetV2SWithInvolution is defined elsewhere)\n",
    "model = EfficientNetV2S_WithInvolution(num_classes=num_classes)  # Dynamically set num_classes\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (e.g., Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the gradient scaler for FP16 precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass using mixed precision (autocast)\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):  # Use autocast for FP16 precision\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient Clipping (before optimizer step)\n",
    "        scaler.unscale_(optimizer)  # Unscale gradients first before clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update the model weights using FP16 gradients and scaling\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        total_preds += labels.size(0)\n",
    "        \n",
    "        # Print training loss and accuracy every 100 steps\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{step}/{len(train_loader)}], Train Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Save model checkpoint every 1000 steps\n",
    "        if step % 1000 == 0:\n",
    "            try:\n",
    "                os.makedirs(f\"model_checkpoints_training_run_{training_run}\")\n",
    "            except:\n",
    "                continue\n",
    "            torch.save(model.state_dict(), f\"model_checkpoints_training_run_{training_run}/saved_model_epoch_{epoch}.pth\")\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_preds / total_preds\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):  # Use autocast for FP16 precision\n",
    "                outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "            # Track accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct_preds / total_preds\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Plot the cumulative training and validation loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label=\"Training Loss\", color='blue', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label=\"Validation Loss\", color='red', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cumulative Loss')\n",
    "plt.title('Cumulative Training and Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdapp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
